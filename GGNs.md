# Graph Neural Networks
This document introduces graph neural networks (GNNs) and their application in this project.

## GNN theory
GNNs, a type of neural network (NN), are used to process data on graphs. GNNs can be used for node, edge and graph level prediction.
Because of the model parameters in NNs, training smoothing methods are used. These methods include: batch normalization, jumping knowledge, and adaptive learning.


## GNNs implementation
Within this project, we use graph layers based on the GAT (graph attention network) [1] from PyTorch Geometric [2].
PyTorch Geometric is a versatile Python library based on PyTorch which can be used for geometric objects (including graphs).
We are using five graph layers with each: 64-elements node vectors, 8 attention heads, batch normalization, and a jumping knowledge network (concatenates all intermediate node embeddings, resulting in ) to the final layer.  
This resulting graph with is pooled using the maximum element of each embedding vector. The maximum pooling operation is chosen because the most important graph nodes decide where the drug binds to the target.
The resulting vector is passes through a dropout layer (probability of node dropout is 0.3) and a fully connected layer with 256 nodes (ReLU activation function). Then, another dropout layer followed by a single node output which the regression value.
In the training, the Adam optimizer is used with a learning rate of 1e-3 and a weight decay of 1e-4. The loss function is the mean squared error (MSE) loss function and the batch size is 64.


## Limitations
Unfair algorithm comparison between RF and GNN because more hand holding is required for GNNs. Specifically, the learning rate was varied from 0.01 to 0.002 and 0.001. And the embedding dimension was changed from 64 to 128. 
Furthermore, prior experience biases the choice of hyperparameters.

The RF model was trained with default parameters.


## Discussion
GNNs are especially useful when working with larger amounts of data. For example, Ziduo et al report finding a MSE of 0.128 from 118,254 ("selected one of the assays containing the largest drug–target pairs") interactions [3] on the KiBA dataset [4].
Thus, extending GNNs to larger multi-target datasets is promising. The multi-task aspect also allows to see the binding prediction to off-targets. Furthermore, there exists explainability methods that help identify substructures of a graph [5]. This can be used to identify the binding site of a drug to a target.
The identification of the binding site of a drug may be extended to the protein which is hypothesized to be of value for docking. Specifically, the identified binding sites may be used in the initialization of the docking algorithm or as a constraint for the docking algorithm.


## References

[1] Veličković, Petar, et al. "Graph attention networks." arXiv preprint arXiv:1710.10903 (2017).

[2] Fey, Matthias, and Jan Eric Lenssen. "Fast graph representation learning with PyTorch Geometric." arXiv preprint arXiv:1903.02428 (2019).

[3] Yang, Ziduo, et al. "MGraphDTA: deep multiscale graph neural network for explainable drug–target binding affinity prediction." Chemical science 13.3 (2022): 816-833.

[4] Making sense of large-scale kinase inhibitor bioactivity data sets: a comparative and integrative analysis. J Chem Inf Model. 2014 Mar 24;54(3):735-43. doi: 10.1021/ci400709d.

[5] Ying, Zhitao, et al. "Gnnexplainer: Generating explanations for graph neural networks." Advances in neural information processing systems 32 (2019).

[//]: # ([6] Battaglia, Peter W., et al. "Relational inductive biases, deep learning, and graph networks." arXiv preprint arXiv:1806.01261 &#40;2018&#41;.)

[//]: # ([1] paragraph text generated by GitHub Copilot)
